# Implementation of a Transformer from scratch
Here I provide my onw implementation of the encoder-decoder transformer architecture based on the orginal ["Attention is all you need"](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper.
I train this model based on some random data and also test its inference process.
This was done with some help from these two great tutorials [1](https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb) and [2](https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch).
You can also find some of my personal notes about Transformers [here](https://michele1993.github.io/assets/pdf/Transformer_Notes.pdf).


<img src="https://github.com/michele1993/Transformer_from_scratch/blob/main/img/EDT_diagram.png" alt="Figure: Encoder Decoder Transformer" width="40%" height="30%">



## Run
Simply run:

```python
python main.py
```

